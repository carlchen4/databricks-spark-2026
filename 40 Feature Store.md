è¿™æ˜¯ä¸ª**éå¸¸å¥½çš„é—®é¢˜**ï¼Œä¹Ÿæ˜¯é“¶è¡Œé‡Œ **MLE / Data Engineer / Model Risk** ç»å¸¸äº‰è®ºçš„ç‚¹ã€‚
æˆ‘ç”¨**ç›´ç™½ + é“¶è¡Œå®æˆ˜è§†è§’**ç»™ä½ è®²æ¸…æ¥šï¼š

---

# ä¸€å¥è¯ç»“è®ºï¼ˆå…ˆç»™ä½ ï¼‰

> **ç›´æ¥å»ºè¡¨ = â€œæˆ‘ç®—å®Œäº†ï¼Œæ”¾è¿™å„¿ä½ è‡ªå·±ç”¨â€**
> **Feature Store = â€œè¿™æ˜¯ä¸€ä¸ªè¢«æ²»ç†ã€å¯å¤ç”¨ã€å¯è¿½æº¯ã€çº¿ä¸Šçº¿ä¸‹ä¸€è‡´çš„ç‰¹å¾äº§å“â€**

---

## ä¸€ã€ç›´æ¥åˆ›å»ºä¸€ä¸ªç‰¹å¾è¡¨ï¼ˆæ™®é€š Delta / Hive è¡¨ï¼‰

### ä½ åœ¨å¹²ä»€ä¹ˆ

* ç”¨ Spark ç®—å®Œç‰¹å¾
* `df.write.saveAsTable("customer_features")`
* ä¸‹æ¸¸æ¨¡å‹ / åˆ†æå¸ˆç›´æ¥æŸ¥è¿™ä¸ªè¡¨

### ä¼˜ç‚¹

* ç®€å•ã€å¿«
* å­¦ä¹ æˆæœ¬ä½
* é€‚åˆä¸€æ¬¡æ€§åˆ†æã€POCã€ä¸´æ—¶æ¨¡å‹

### åœ¨é“¶è¡Œé‡Œçš„çœŸå®é—®é¢˜

1ï¸âƒ£ **ç‰¹å¾å®šä¹‰åˆ†æ•£**

* ä¸åŒå›¢é˜Ÿå„ç®—ä¸€ä»½
* â€œ30 å¤©äº¤æ˜“æ¬¡æ•°â€å‡ºç° 5 ä¸ªç‰ˆæœ¬

2ï¸âƒ£ **çº¿ä¸Š / çº¿ä¸‹ä¸ä¸€è‡´**

* è®­ç»ƒç”¨ A è¡¨
* å®æ—¶é¢„æµ‹è‡ªå·±å†ç®—ä¸€é â†’ **training-serving skew**

3ï¸âƒ£ **ä¸å¯å®¡è®¡**

* Model Risk é—®ï¼š

  > è¿™ä¸ªç‰¹å¾ä»€ä¹ˆæ—¶å€™ç®—çš„ï¼Ÿç”¨çš„ä»€ä¹ˆæ•°æ®ï¼Ÿ
* ä½ ï¼šğŸ˜¶

4ï¸âƒ£ **éš¾å¤ç”¨**

* ä¸‹ä¸€ä¸ªæ¨¡å‹åˆ copy ä¸€ä»½ Spark ä»£ç 

---

## äºŒã€Feature Store åœ¨å¹²ä»€ä¹ˆï¼ˆæœ¬è´¨ï¼‰

> **Feature Store = ç‰¹å¾çš„â€œæ³¨å†Œä¸­å¿ƒ + ç”Ÿå‘½å‘¨æœŸç®¡ç†â€**

å®ƒä¸åªæ˜¯â€œå­˜æ•°æ®â€ï¼Œè€Œæ˜¯ **ç®¡ç†ç‰¹å¾**ã€‚

---

## ä¸‰ã€Feature Store æ¯”â€œç›´æ¥å»ºè¡¨â€å¤šäº†ä»€ä¹ˆï¼Ÿ

### 1ï¸âƒ£ ç‰¹å¾æ˜¯â€œè¢«æ³¨å†Œçš„â€

* æ¯ä¸ªç‰¹å¾æœ‰ï¼š

  * åå­—
  * æè¿°
  * è®¡ç®—é€»è¾‘
  * ä¸»é”®ï¼ˆcustomer_id / account_idï¼‰
  * æ—¶é—´æˆ³

ğŸ‘‰ ä¸å†æ˜¯â€œéšä¾¿ä¸€ä¸ªåˆ—â€

---

### 2ï¸âƒ£ è‡ªåŠ¨é˜²æ­¢ Data Leakageï¼ˆé“¶è¡Œéå¸¸é‡è¦ï¼‰

Feature Store ä¼šï¼š

* æŒ‰ **event_time** è‡ªåŠ¨åš point-in-time join
* ç¡®ä¿ï¼š

  * è®­ç»ƒæ—¶åªçœ‹åˆ°å½“æ—¶èƒ½çœ‹åˆ°çš„æ•°æ®

ğŸ‘‰ ç›´æ¥å»ºè¡¨ **éå¸¸å®¹æ˜“æ³„éœ²æœªæ¥ä¿¡æ¯**

---

### 3ï¸âƒ£ è®­ç»ƒ & çº¿ä¸Šé¢„æµ‹ç”¨çš„æ˜¯ **åŒä¸€å¥—ç‰¹å¾**

* è®­ç»ƒï¼š

  * Spark æ‰¹é‡è¯» Feature Store
* çº¿ä¸Šï¼š

  * å®æ—¶æœåŠ¡ä» Online Store æ‹¿åŒä¸€ä¸ªç‰¹å¾

ğŸ‘‰ ç›´æ¥å»ºè¡¨ï¼š

> â€œæˆ‘è®­ç»ƒæ—¶ç®—ä¸€å¥—ï¼Œçº¿ä¸Šä½ å†ç®—ä¸€å¥—å§â€

---

### 4ï¸âƒ£ é“¶è¡Œåˆè§„ / å®¡è®¡å‹å¥½ï¼ˆModel Risk æœ€çˆ±ï¼‰

Feature Store å¯ä»¥å›ç­”ï¼š

* è¿™ä¸ªç‰¹å¾ï¼š

  * è°åˆ›å»ºçš„ï¼Ÿ
  * ä»€ä¹ˆæ—¶å€™ï¼Ÿ
  * ç”¨åœ¨å“ªäº›æ¨¡å‹ï¼Ÿ
  * ä¸Šæ¸¸æ•°æ®æºæ˜¯ä»€ä¹ˆï¼Ÿ

ğŸ‘‰ **OSFI / OCC / Model Risk** éƒ½å…³å¿ƒè¿™ä¸ª

---

### 5ï¸âƒ£ çœŸæ­£â€œå¤ç”¨â€çš„èƒ½åŠ›

* Fraud æ¨¡å‹ç®—è¿‡ï¼š

  * `txn_count_30d`
* Marketing æ¨¡å‹ï¼š

  * ç›´æ¥å¤ç”¨
* ä¸éœ€è¦é‡æ–°å†™ Spark

---

## å››ã€Feature Store vs ç›´æ¥å»ºè¡¨ï¼ˆé“¶è¡Œè§†è§’æ€»ç»“ï¼‰

### ç›´æ¥å»ºè¡¨

* æ›´åƒï¼š**ä¸´æ—¶ç»“æœè¡¨**
* é€‚åˆï¼š

  * ä¸€æ¬¡æ€§åˆ†æ
  * æ¢ç´¢æ€§å»ºæ¨¡
  * POC

### Feature Store

* æ›´åƒï¼š**ç”Ÿäº§çº§ç‰¹å¾èµ„äº§**
* é€‚åˆï¼š

  * å¤šæ¨¡å‹å…±äº«
  * é•¿æœŸä½¿ç”¨
  * æœ‰å®¡è®¡ / åˆè§„è¦æ±‚çš„é“¶è¡Œåœºæ™¯

---

## äº”ã€ä»€ä¹ˆæ—¶å€™ä½  **ä¸éœ€è¦** Feature Storeï¼Ÿ

è¯´å®è¯ï¼Œåœ¨é“¶è¡Œä¹Ÿä¸æ˜¯æ‰€æœ‰æ—¶å€™éƒ½è¦ï¼š

* å•ä¸€æ¨¡å‹
* çŸ­æœŸé¡¹ç›®
* æ— å®æ—¶é¢„æµ‹
* æ— å¤ç”¨éœ€æ±‚

ğŸ‘‰ **ç›´æ¥ Delta è¡¨æ›´çœäº‹**

---

## å…­ã€CIBC / é“¶è¡ŒçœŸå®è½åœ°å¥—è·¯ï¼ˆä½ ä¼šçœ‹åˆ°çš„ï¼‰

1ï¸âƒ£ æ—©æœŸ

* Spark ç®—ç‰¹å¾ â†’ Delta è¡¨

2ï¸âƒ£ ç¨³å®šå

* é«˜ä»·å€¼ç‰¹å¾
* é«˜é¢‘å¤ç”¨
* é£é™© / å®¢æˆ·ç”»åƒç±»

ğŸ‘‰ **è¿å…¥ Feature Store**

---

## ä¸ƒã€ç»™ä½ ä¸€å¥â€œé¢è¯•çº§æ€»ç»“â€

> *â€œFeature Store is not about storage. Itâ€™s about feature governance, reuse, and training-serving consistency.â€*

---

æ˜ç™½äº†ï¼ŒCarlï¼Œæˆ‘ç»™ä½ å†™ä¸€ä¸ªå®Œæ•´çš„ **Databricks Feature Store ç¤ºä¾‹ä»£ç **ï¼Œå±•ç¤ºå¦‚ä½•ä»é“¶è¡Œäº¤æ˜“æ•°æ®åˆ›å»ºä¸€ä¸ªç‰¹å¾è¡¨ï¼Œå¹¶å†™å…¥ Feature Storeã€‚

---

### ä¾‹å­åœºæ™¯

* åŸå§‹äº¤æ˜“è¡¨ï¼š`transactions`

  * åˆ—ï¼š`account_id`ã€`txn_time`ã€`amount`ã€`status`
* ç‰¹å¾è¦è®¡ç®—ï¼š

  1. **è¿‡å» 30 å¤©äº¤æ˜“æ€»é¢** â†’ `last_30d_amount`
  2. **æœ€è¿‘ 3 ç¬”äº¤æ˜“é‡‘é¢æ€»å’Œ** â†’ `last_3_txn_amount`

---

### Step 1ï¼šå¯¼å…¥åº“ & åˆå§‹åŒ– Feature Store å®¢æˆ·ç«¯

```python
from pyspark.sql import Window
from pyspark.sql.functions import sum, col
from databricks.feature_store import FeatureStoreClient

# åˆå§‹åŒ– Feature Store å®¢æˆ·ç«¯
fs = FeatureStoreClient()
```

---

### Step 2ï¼šè¯»å–äº¤æ˜“æ•°æ®

```python
df = spark.read.parquet("/mnt/data/transactions")  # å‡è®¾æ•°æ®å­˜å‚¨åœ¨ Databricks çš„æ•°æ®æ¹–
```

---

### Step 3ï¼šè®¡ç®—ç‰¹å¾

```python
# çª—å£å‡½æ•°
window_30d = Window.partitionBy("account_id").orderBy("txn_time").rowsBetween(-29, 0)
window_3txns = Window.partitionBy("account_id").orderBy("txn_time").rowsBetween(-2, 0)

# è®¡ç®—ç‰¹å¾
df_features = (
    df.withColumn("last_30d_amount", sum(col("amount")).over(window_30d))
      .withColumn("last_3_txn_amount", sum(col("amount")).over(window_3txns))
)
```

---

### Step 4ï¼šåˆ›å»º Feature Store è¡¨

```python
fs.create_table(
    name="bank_customer_features",
    primary_keys="account_id",
    df=df_features,
    description="Customer transaction features for risk modeling"
)
```

* `primary_keys="account_id"`ï¼šæ¯ä¸ªè´¦æˆ·æ˜¯ç‰¹å¾è¡¨çš„ä¸»é”®
* `df=df_features`ï¼šæ•°æ®æ¥æº
* `description`ï¼šç‰¹å¾è¡¨æè¿°

---

### Step 5ï¼šè¯»å–ç‰¹å¾è¡¨ç”¨äºè®­ç»ƒ

```python
training_df = fs.read_table("bank_customer_features")
training_df.display()
```

---

### Step 6ï¼šåœ¨çº¿é¢„æµ‹ï¼ˆä½å»¶è¿ŸæŸ¥è¯¢ï¼‰

```python
online_features = fs.get_online_features(
    table_name="bank_customer_features",
    keys={"account_id": ["ACC001", "ACC002"]}
)

online_features.show()
```

