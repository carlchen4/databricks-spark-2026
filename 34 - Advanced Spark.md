å¥½ï¼Œè¿™äº›**æ­£æ˜¯ Databricks / Spark çœŸÂ·è¿›é˜¶å¿…å­¦ç‚¹**ã€‚æˆ‘ç»™ä½ ä¸€å¥—**â€œMLE + é“¶è¡Œåœºæ™¯å‹å¥½â€**çš„è®²è§£ï¼Œ**ä¸å•°å—¦ã€ç›´ç»™ã€å¸¦ä¾‹å­ã€å¸®ä½ å»ºç«‹æ„Ÿè§‰**ã€‚

---

# ä¸€ã€Windowï¼š`rowsBetween` vs `rangeBetween`

ğŸ‘‰ **åšæ—¶é—´åºåˆ—ã€äº¤æ˜“æ»šåŠ¨ç»Ÿè®¡å¿…ä¼š**

## 1ï¸âƒ£ `rowsBetween`ï¼ˆæŒ‰â€œè¡Œæ•°â€ï¼‰

**ä¸çœ‹å€¼ï¼Œåªçœ‹å‰åå¤šå°‘è¡Œ**

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import sum

w = Window.partitionBy("account_id") \
          .orderBy("transaction_date") \
          .rowsBetween(-2, 0)

df.withColumn("sum_last_3_txn", sum("amount").over(w))
```

**å«ä¹‰**

* å½“å‰è¡Œ + å‰ 2 è¡Œ
* å¸¸ç”¨äºï¼š**æœ€è¿‘ N ç¬”äº¤æ˜“**

ğŸ“Œ é“¶è¡Œä¾‹å­

> æœ€è¿‘ **3 ç¬”äº¤æ˜“æ€»é‡‘é¢**

---

## 2ï¸âƒ£ `rangeBetween`ï¼ˆæŒ‰â€œå€¼èŒƒå›´â€ï¼‰

**çœ‹ orderBy çš„å€¼èŒƒå›´ï¼ˆé€šå¸¸æ˜¯æ—¶é—´ï¼‰**

```python
from pyspark.sql.functions import col

w = Window.partitionBy("account_id") \
          .orderBy(col("txn_ts").cast("long")) \
          .rangeBetween(-7*86400, 0)
```

**å«ä¹‰**

* æœ€è¿‘ **7 å¤©å†…** çš„äº¤æ˜“
* æ—¶é—´ä¸è¿ç»­ä¹Ÿæ²¡é—®é¢˜

ğŸ“Œ é“¶è¡Œä¾‹å­

> æœ€è¿‘ **7 å¤©æ¶ˆè´¹æ€»é¢ï¼ˆAML / é£æ§ï¼‰**

---

## å¯¹æ¯”æ€»ç»“

| å¯¹æ¯”     | rowsBetween | rangeBetween |
| ------ | ----------- | ------------ |
| ä¾æ®     | è¡Œæ•°          | å€¼ï¼ˆæ—¶é—´ / æ•°å€¼ï¼‰   |
| æ˜¯å¦è¿ç»­   | å¿…é¡»          | ä¸è¦æ±‚          |
| å¸¸ç”¨åœºæ™¯   | æœ€è¿‘ N ç¬”      | æœ€è¿‘ N å¤©       |
| é£æ§/AML | âŒ           | âœ…            |

---

# äºŒã€Catalyst Optimizer æ˜¯ä»€ä¹ˆ

ğŸ‘‰ **Spark SQL çš„â€œå¤§è„‘â€**

**ä¸€å¥è¯**

> Catalyst ä¼šæŠŠä½ å†™çš„ SQL / DataFrame **å·å·æ”¹å†™æˆæ›´å¿«çš„æ‰§è¡Œè®¡åˆ’**

## Catalyst ä¼šå¹²å•¥ï¼Ÿ

* Predicate Pushdown
* Column Pruning
* Reorder Filter / Join
* Simplify Expression

ä½ å†™ï¼š

```python
df.filter(col("amount") > 100).select("account_id", "amount")
```

Catalyst å¯èƒ½å˜æˆï¼š

```text
â†’ å…ˆ filter
â†’ å†åªè¯» account_id, amount ä¸¤åˆ—
```

ğŸ“Œ **ä½ ä¸ç”¨æ‰‹å†™ä¼˜åŒ–ï¼Œä½†è¦â€œé…åˆå®ƒâ€**

---

# ä¸‰ã€Predicate Pushdownï¼ˆè¶…çº§é‡è¦ï¼‰

ğŸ‘‰ **é“¶è¡Œæ•°æ®å¿…è€ƒç‚¹**

**æ„æ€**

> æŠŠ filter **æ¨åˆ°æ•°æ®æºå±‚ï¼ˆParquet / Deltaï¼‰**

```python
df.filter("txn_date >= '2024-01-01'")
```

å¦‚æœï¼š

* Parquet / Delta
* é UDF
* ç®€å•æ¯”è¾ƒ

ğŸ‘‰ Spark **åªè¯»æ»¡è¶³æ¡ä»¶çš„ row group**

âŒ ä¸èƒ½ä¸‹æ¨ï¼š

```python
df.filter(my_udf(col("amount")) > 0)
```

---

## ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

| åœºæ™¯   | åæœ     |
| ---- | ------ |
| å¯ä¸‹æ¨  | å°‘ IOï¼Œå¿« |
| ä¸å¯ä¸‹æ¨ | å…¨è¡¨æ‰«æ   |

ğŸ“Œ é“¶è¡Œï¼š**10 å¹´äº¤æ˜“è¡¨ â†’ åªæ‰« 1 ä¸ªæœˆ**

---

# å››ã€Z-Orderï¼ˆDatabricks ç‰¹æœ‰ï¼‰

ğŸ‘‰ **Delta Lake çš„ç‰©ç†æ’åº**

```sql
OPTIMIZE transactions
ZORDER BY (account_id, txn_date)
```

**æ•ˆæœ**

* ç›¸ä¼¼å€¼ç‰©ç†é è¿‘
* å‡å°‘æ–‡ä»¶æ‰«æ

ğŸ“Œ é“¶è¡Œæ¨è Z-Order

* `account_id`
* `customer_id`
* `txn_date`

â— Z-Order â‰  partition
ğŸ‘‰ **æ˜¯è¡¥å……å…³ç³»**

---

# äº”ã€AS OF / Time Travel

ğŸ‘‰ **å®¡è®¡ & å›æº¯ç¥æŠ€**

```sql
SELECT * FROM transactions VERSION AS OF 123
```

```sql
SELECT * FROM transactions TIMESTAMP AS OF '2024-10-01'
```

ğŸ“Œ é“¶è¡Œç”¨æ³•

* æ¨¡å‹è®­ç»ƒå¤ç°
* å®¡è®¡ / åˆè§„
* å›æ»šé”™è¯¯æ•°æ®

---

# å…­ã€Photon Engineï¼ˆDatabricks é¢è¯•å¸¸é—®ï¼‰

ğŸ‘‰ **C++ å‘é‡åŒ–æ‰§è¡Œå¼•æ“**

**ä¸€å¥è¯**

> Photon = Spark SQL çš„â€œå¤–æŒ‚åŠ é€Ÿå™¨â€

* ç”¨ C++ é‡å†™æ‰§è¡Œç®—å­
* å‘é‡åŒ– + SIMD
* å¯¹ SQL / DF è‡ªåŠ¨ç”Ÿæ•ˆ

ğŸ“Œ åŠ é€Ÿæ˜æ˜¾åœºæ™¯

* Aggregation
* Join
* Window
* Scan Parquet / Delta

---

# ä¸ƒã€Shuffle æ˜¯ä»€ä¹ˆï¼ˆå¿…ä¼šï¼‰

ğŸ‘‰ **Spark æœ€è´µçš„æ“ä½œ**

**å®šä¹‰**

> æ•°æ®ä»ä¸€ä¸ª executor **è·¨ç½‘ç»œ** å‘é€åˆ°å¦ä¸€ä¸ª

å‘ç”Ÿåœ¨ï¼š

* `groupBy`
* `join`
* `orderBy`
* `repartition`

ğŸ“Œ ç‰¹ç‚¹

* æ…¢
* å ç½‘ç»œ
* å ç£ç›˜

---

# å…«ã€`repartition` vs `coalesce`

ğŸ‘‰ **æ§åˆ¶ Shuffle çš„æ ¸å¿ƒæ‰‹æ®µ**

## 1ï¸âƒ£ repartitionï¼ˆæœ‰ Shuffleï¼‰

```python
df.repartition(200, "account_id")
```

* **æ‰“ä¹±æ•°æ®**
* å¯å¢ / å‡ partition
* æˆæœ¬é«˜

ğŸ“Œ ç”¨äºï¼š

* Join å‰
* æ•°æ®å‡åŒ€åŒ–

---

## 2ï¸âƒ£ coalesceï¼ˆæ—  Shuffleï¼‰

```python
df.coalesce(20)
```

* **åªåˆå¹¶**
* ä¸é‡æ–°åˆ†å¸ƒ
* å¾ˆå¿«

ğŸ“Œ ç”¨äºï¼š

* å†™æ•°æ®å‰å‡å°‘å°æ–‡ä»¶

---

## å¯¹æ¯”

|                | repartition | coalesce |
| -------------- | ----------- | -------- |
| æ˜¯å¦ Shuffle     | âœ…           | âŒ        |
| æ˜¯å¦å‡åŒ€           | âœ…           | âŒ        |
| æ˜¯å¦å¯å¢ partition | âœ…           | âŒ        |
| å†™æ•°æ®å‰           | âŒ           | âœ…        |

---

# ä¹ã€Data Skewï¼ˆé“¶è¡Œæ•°æ®å¸¸è§å‘ï¼‰

ğŸ‘‰ **ä¸€éƒ¨åˆ† task ç‰¹åˆ«æ…¢**

## å…¸å‹åœºæ™¯

* `account_id = 000001` æœ‰ 1 äº¿äº¤æ˜“
* å…¶ä»–è´¦å·åªæœ‰å‡ ç™¾æ¡

ç»“æœï¼š

* ä¸€ä¸ª task è·‘ 30 åˆ†é’Ÿ
* å…¶ä»–æ—©å°±ç»“æŸ

---

## è§£å†³æ–¹æ¡ˆ

### 1ï¸âƒ£ Saltingï¼ˆæœ€ç»å…¸ï¼‰

```python
df.withColumn("salt", rand())
```

### 2ï¸âƒ£ AQEï¼ˆDatabricks æ¨èï¼‰

```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

### 3ï¸âƒ£ æ‹†å¤§ key

* çƒ­è´¦æˆ·å•ç‹¬å¤„ç†

---

# åã€ç»™ä½ ä¸€å¼  **MLE å¿…ä¼šæ¸…å•ï¼ˆé“¶è¡Œï¼‰**

| ä¸»é¢˜                     | æ˜¯å¦å¿…ä¼š |
| ---------------------- | ---- |
| Windowï¼ˆrangeBetweenï¼‰   | âœ…    |
| Catalyst åŸç†            | âœ…    |
| Predicate Pushdown     | âœ…    |
| Z-Order                | âœ…    |
| Photon                 | âœ…    |
| Shuffle                | âœ…    |
| repartition / coalesce | âœ…    |
| Data Skew              | âœ…    |

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ä¸‹ä¸€æ­¥ç›´æ¥åšğŸ‘‡

* âœ… **ä¸€å¥— Databricks é¢è¯•é¢˜ï¼ˆå«é™·é˜±ï¼‰**
* âœ… **AML / é£æ§ Spark å®æˆ˜ Notebook**
* âœ… **Spark æ‰§è¡Œè®¡åˆ’ explain() æ‹†è§£**

ä½ æƒ³å…ˆå“ªä¸€ä¸ªï¼Ÿ
